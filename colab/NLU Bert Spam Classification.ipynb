{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN1TalVZ846RTaoWUWuotCH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"a3935192880e4656a32352a51c2fd58f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f82c231a022e49d89c7d8911f5643f6e","IPY_MODEL_837ad84c194e4e8b867fc878897d25ca","IPY_MODEL_31cb5eb5250942de84c7b2200abf2f9d"],"layout":"IPY_MODEL_3057a124354d45e6ac22913ef9765a7f"}},"f82c231a022e49d89c7d8911f5643f6e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_68faa7641de1461d94b95a8dc9fec586","placeholder":"‚Äã","style":"IPY_MODEL_f4e799a8c7ab49f3a68bd95e50bcdc6c","value":"Skipping the first batches: 100%"}},"837ad84c194e4e8b867fc878897d25ca":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1aec608cdf124e64bb38601f875b8b37","max":178,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6aa89e216f9d4833ab4a621f4320318a","value":178}},"31cb5eb5250942de84c7b2200abf2f9d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5f94c20d3a1c4004a556bf276f0bea40","placeholder":"‚Äã","style":"IPY_MODEL_796c75d93bd64994b2b6290025df7729","value":" 178/178 [00:01&lt;00:00, 202.62it/s]"}},"3057a124354d45e6ac22913ef9765a7f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"68faa7641de1461d94b95a8dc9fec586":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f4e799a8c7ab49f3a68bd95e50bcdc6c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1aec608cdf124e64bb38601f875b8b37":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6aa89e216f9d4833ab4a621f4320318a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5f94c20d3a1c4004a556bf276f0bea40":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"796c75d93bd64994b2b6290025df7729":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# Install and import the required packages"],"metadata":{"id":"vEEAEbIXNI6K"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bcrxQkKP84lR","executionInfo":{"status":"ok","timestamp":1682523942166,"user_tz":-60,"elapsed":5743,"user":{"displayName":"Tony Lay","userId":"12991364819392579390"}},"outputId":"232a95ce-0dcf-4ddb-c4eb-0e774bfbe468"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (1.24.3)\n","Requirement already satisfied: transformers==4.23.1 in /usr/local/lib/python3.9/dist-packages (4.23.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (1.5.3)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (1.2.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (2.0.0+cu118)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (3.7.1)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.9/dist-packages (2.11.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.9/dist-packages (from transformers==4.23.1) (0.14.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers==4.23.1) (4.65.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers==4.23.1) (3.11.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers==4.23.1) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.23.1) (6.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.23.1) (0.13.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.23.1) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers==4.23.1) (2.27.1)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas) (2022.7.1)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (1.10.1)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (3.1.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch) (3.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.5.0)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch) (2.0.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch) (3.1.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch) (16.0.1)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch) (3.25.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (1.4.4)\n","Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (5.12.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (0.11.0)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (8.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (3.0.9)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (1.0.7)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (4.39.3)\n","Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (2023.4.0)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from datasets) (3.2.0)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (9.0.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from datasets) (0.70.14)\n","Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.18.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets) (3.8.4)\n","Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.3.6)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (2.0.12)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.9.2)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.3.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib) (3.15.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.23.1) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.23.1) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.23.1) (3.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch) (1.3.0)\n"]}],"source":["!pip install numpy transformers==4.23.1 pandas scikit-learn torch matplotlib datasets"]},{"cell_type":"code","source":["import transformers\n","import json\n","import numpy as np\n","import matplotlib\n","import pandas as pd\n","import torch \n","from sklearn.model_selection import train_test_split\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, DistilBertForSequenceClassification\n","from transformers import Trainer, TrainingArguments\n","from datasets import load_metric"],"metadata":{"id":"f_VO95Ub-J-L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"WDxQq_JJ-Sda","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682523944462,"user_tz":-60,"elapsed":2304,"user":{"displayName":"Tony Lay","userId":"12991364819392579390"}},"outputId":"fd3dce5a-03c0-45a5-d0f4-afac08dbc049"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["# Load pre-trained model, define data folder path and dataset file name"],"metadata":{"id":"NAAVdLdxNRX-"}},{"cell_type":"code","source":["model_shortcut = \"distilbert-base-cased\"\n","tokenizer = AutoTokenizer.from_pretrained(model_shortcut)\n","model = AutoModelForSequenceClassification.from_pretrained(model_shortcut, num_labels=3)\n","\n","DATA_PATH = '/content/drive/MyDrive/NLU Bert Spam Classification/data/'\n","DATASET_FILENAME = 'combinedmorespam-dataset-labelled'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rYPyHYFXMjnE","executionInfo":{"status":"ok","timestamp":1682523946825,"user_tz":-60,"elapsed":2368,"user":{"displayName":"Tony Lay","userId":"12991364819392579390"}},"outputId":"f665d837-b0f6-4237-e21b-9ff37ffe0081"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-cased/snapshots/4dc145c5bd4fdb672dcded7fdc1efd6c2bc55992/config.json\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-cased\",\n","  \"activation\": \"gelu\",\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.23.1\",\n","  \"vocab_size\": 28996\n","}\n","\n","loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-cased/snapshots/4dc145c5bd4fdb672dcded7fdc1efd6c2bc55992/vocab.txt\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-cased/snapshots/4dc145c5bd4fdb672dcded7fdc1efd6c2bc55992/tokenizer.json\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at None\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-cased/snapshots/4dc145c5bd4fdb672dcded7fdc1efd6c2bc55992/tokenizer_config.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-cased/snapshots/4dc145c5bd4fdb672dcded7fdc1efd6c2bc55992/config.json\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-cased\",\n","  \"activation\": \"gelu\",\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.23.1\",\n","  \"vocab_size\": 28996\n","}\n","\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-cased/snapshots/4dc145c5bd4fdb672dcded7fdc1efd6c2bc55992/config.json\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-cased\",\n","  \"activation\": \"gelu\",\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.23.1\",\n","  \"vocab_size\": 28996\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-cased/snapshots/4dc145c5bd4fdb672dcded7fdc1efd6c2bc55992/pytorch_model.bin\n","Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["def get_topics_from_csv(file_path):\n","  \"\"\"Retrieves list of topic keywords from a topic csv file\n","  Args:\n","    file_path (str): file path to topic csv file\n","  Returns:\n","    topic_dict (dict): dictionary mapping from video_id to a list of topic keywords\n","  \"\"\"\n","  # Dataframe of csv file\n","  df = pd.read_csv(file_path)\n","\n","  # Create a dictionary mapping from video_id to their topics\n","  topic_dict = dict()\n","  video_ids = list(df['video_id'].unique())\n","  for video_id in video_ids:\n","    query = f\"video_id=='{video_id}'\"\n","    topic_keywords = df.query(query)['topic_keywords'].iloc[0]\n","    topic_dict[video_id] = topic_keywords\n","\n","  return topic_dict\n","\n","def text_to_label(cls):\n","  \"\"\"Converts class to an integer label\n","  Args:\n","    cls (str): class\n","  Returns:\n","    label (int): integer label in set {0,1,2}, returns -1 if there is an unidentifiable class\n","  \"\"\"\n","  if cls == \"spam\":\n","    return 0\n","  elif cls == \"neutral\":\n","    return 1\n","  elif cls == \"ham\":\n","    return 2\n","  else:\n","    return -1\n","\n","def parse_comment(comment):\n","  \"\"\"Parses a comment in a format suitable for the custom tokenizer\n","  Args:\n","    comment (str): raw comment string in the comment retrieval format\n","  Returns:\n","    parsed_comment (str): parsed string\n","  \"\"\"  \n","  if '[REPLY]' in comment:\n","    return comment\n","  else:\n","    return f\"[MAIN] {comment}\"\n","    \n","def get_comments_from_csv(file_path):\n","  \"\"\"Retrieve comment data from a specified .csv file\n","  Args:\n","      file_name (str): .csv file name\n","  Returns:\n","      comments_by_videoid (dict): dictionary mapping from video ID to its list of comments\n","      video_name_dict (dict): dictionary mapping from video ID its video name\n","  \"\"\"\n","  if file_path[-4:] != '.csv':\n","      file_path += '.csv'\n","\n","  df = pd.read_csv(file_path)\n","  info_dict = dict()\n","  video_ids = list(df['video_id'].unique())\n","  for video_id in video_ids:\n","      query = f\"video_id=='{video_id}'\"\n","      comments = list(df.query(query)['comment'])\n","      usernames = list(df.query(query)['username'])\n","      labels = list(df.query(query)['class'])\n","\n","      info = []\n","      for i in range(0,len(comments)):\n","        label = text_to_label(labels[i])\n","        if label == -1:\n","          continue\n","        info.append([f\"[USER] {usernames[i]} {parse_comment(comments[i])} \",label])\n","\n","      info_dict[video_id] = info\n","      \n","  return info_dict\n","\n","def create_dataset(data_path,dataset_filename):\n","  \"\"\"Create a dataset from a .csv file\n","  Args:\n","      data_path (str): path to the data folder\n","      dataset_filename (str): name of the dataset file\n","  Returns:\n","      dataset (list): dataset where each data item contains a sentence pair (comment, topic keywords)\n","      labels (list): list of labels corresponding to the indices in the dataset\n","  \"\"\"\n","  topic_dict = get_topics_from_csv(f\"{data_path}/topics/{dataset_filename}-topics.csv\")\n","  info_dict = get_comments_from_csv(f\"{data_path}/labelled/{dataset_filename}\")\n","\n","  video_ids = topic_dict.keys()\n","  dataset = []\n","  labels = []\n","  for video_id in video_ids:\n","    for comment_data in info_dict[video_id]:\n","      data_sample = (comment_data[0],topic_dict[video_id])\n","      dataset.append(data_sample)\n","      labels.append(comment_data[1])\n","\n","  return dataset, labels\n","\n","# Add custom tokens to tokenizer\n","num_added_toks = tokenizer.add_tokens(['[USER]','[MAIN]','[REPLY]'], special_tokens=True)\n","model.resize_token_embeddings(len(tokenizer))\n","\n","def tokenize_data(data):\n","  comments, topic_keywords = zip(*data)\n","  return tokenizer(comments,topic_keywords, padding=\"max_length\", truncation='only_first')"],"metadata":{"id":"uoReArEWPUDD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Import dataset from file path"],"metadata":{"id":"kY8R1UHMOZXb"}},{"cell_type":"code","source":["# Define dataset and labels\n","dataset, labels = create_dataset(DATA_PATH,DATASET_FILENAME)\n","\n","# Split into train and test data splits\n","train_data, test_data, train_labels, test_labels = train_test_split(dataset, labels, test_size=0.2, random_state=50)\n","\n","# Encode train and test data\n","train_encodings = tokenize_data(train_data)\n","test_encodings = tokenize_data(test_data)\n"],"metadata":{"id":"iXOkz5GJOAVm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Show examples of spam\n","idx = 0\n","while train_labels[idx] != 0 or test_labels[idx] != 0:\n","  idx += 1\n","print(train_labels[idx])\n","print(tokenizer.decode(train_encodings[idx].ids))\n","print(test_labels[idx])\n","print(tokenizer.decode(test_encodings[idx].ids))\n","\n","# Show other examples\n","idx = 12\n","print(train_labels[idx])\n","print(tokenizer.decode(train_encodings[idx].ids))\n","print(test_labels[idx])\n","print(tokenizer.decode(test_encodings[idx].ids))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-kCyr_srvTCX","executionInfo":{"status":"ok","timestamp":1682523960889,"user_tz":-60,"elapsed":5,"user":{"displayName":"Tony Lay","userId":"12991364819392579390"}},"outputId":"601864e6-7c80-446e-ea7b-7efc3bd60fa0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n","[CLS] [USER] [UNK] [UNK] = [UNK] [UNK] [UNK] [UNK] [MAIN] * Let ‚Äô s just take a minute to appreciate how much time and work he put into these videos? * * It's unbelievable, and I think they deserve a lot more than that * [UNK] [UNK] : 08 [SEP] pie, kind, kreekcraft, anti, lol, dedication, baller, ever, video, content, honest, roblox, unbelievable, appreciate, dad, bruh, lot, kid, biggest, reply, vantherz, work, hilarious, much, touch, take, hell, update, people, cheat, grass, let, least, respect, webcam, love, cutie, problem, bot, bro, sssniperwolf, time, day, main, fake, minute, recent, moment [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","0\n","[CLS] [USER] 5k Subs Challenge no Videos [MAIN] You guys look like you're having so much fun with this video [SEP] match, opponent, competitive, technique, human, long, tilt, friend, compulsory, service, cool, hell, shit, public, practice, et, construction, main, arm, day, age, watch, strength, sore, year, tran, strong, swear, epic, bunch, way, furniture, rest, haha, pride, wrestler, pro, love, workout, great, win, fun, culture, guy, wrestling, climbing, man, tomorrow, group, challenges, amazing, one, humble, hard, people, behavior, sure, interesting, big, eachother, good, time, magnus midtb√∏, help, army, video, nice, peace, lot, reply, elbow, worker, smart, dude, table, heal, youtube, gap, tecnique, expert [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","2\n","[CLS] [USER] JackieZ [MAIN] Jimmy is the Best Youtuber. No Debate. [SEP] votre, team, mother, well, tanta, hat, –æ–∑–≤—É—á–∫—É, hacer, massive, usd, crew, —Ä–æ–ª–∏–∫–æ–≤, increible, bed, –±–∏—Å—Ç, hacen, lucky, peine, persona, sin, dangerous, que, mrbeast, special, entire, rompen, day, main, air, available, history, person, –∑–∞, joy, —Å—Ç–∞—Ä–∞–Ω–∏—è, –Ω–∏—á—Ç–æ, year, content, estoy, planting, sigas, respect, fresh, love, flight, motivo, great, fun, crazy, participe, job, success, man, ticket, –ª—É—á—à–∏–∏, beach, existir, ultimo, verdad, amazing, mxm, keep, earth, mom, people, million, sure, —ç—Ç–æ–≥–æ, blindness, vraiment, puede, suscrito, good, time, este, awesome, camera, excelente, vs, breath, page, –º–∏—Å—Ç–µ—Ä, humor, level, happiness, many, thrill, plane, —Å–ø–∞—Å–∏–±–æ, vous, much [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","2\n","[CLS] [USER] PT Approved [MAIN] These videos are great. Straight to the point and very well edited & put together. I found you channel about a month ago and really do look forward to them when they pop up on my feed. I've always had problems with the shooting part of my game and never really ironed that part out. Keep it up, you deserve the subs coming you way! [SEP] fundamental, point, motion, journey, new, man, team, help, year, leg, power, thank, knee, underrated, video, content, fold, kick, technique, striker, lot, reply, mind, single, change, tutorial, straight, foot, free, work, amazing, pls, bullshit, ball, training, boot, step, accurate, channel, way, kicking, sure, energy, the football folk, true, part, football, huge, shooting, game, next, high, glad, practice, shoot, regardless, youtube, great, favorite, agility, quality, field, bro, short, ver, good, style, main, arm, thing, note, mate, sub [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"]}]},{"cell_type":"markdown","source":["# Prepare dataset and Trainer arguments for training"],"metadata":{"id":"rfRuoEw-Ofil"}},{"cell_type":"code","source":["# Define PyTorch Dataset class\n","class CustomDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","train_dataset = CustomDataset(train_encodings, train_labels)\n","test_dataset = CustomDataset(test_encodings, test_labels)"],"metadata":{"id":"joy1CChmdym_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_args = TrainingArguments(\n","    optim=\"adamw_torch\",\n","    output_dir='/content/drive/MyDrive/NLU Bert Spam Classification/results',          # output directory\n","    num_train_epochs=3,              # total number of training epochs\n","    per_device_train_batch_size=16,  # batch size per device during training\n","    per_device_eval_batch_size=64,   # batch size for evaluation\n","    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n","    weight_decay=0.01,               # strength of weight decay\n","    logging_dir='/content/drive/MyDrive/NLU Bert Spam Classification/logs',            # directory for storing logs\n","    logging_steps=100,\n","    save_steps=100,\n","    learning_rate=2e-5\n","    eval_steps=100\n",")\n","\n","trainer = Trainer(\n","    model=model,                         # the instantiated ü§ó Transformers model to be trained\n","    args=training_args,                  # training arguments, defined above\n","    train_dataset=train_dataset,         # training dataset\n","    eval_dataset=test_dataset             # evaluation dataset\n",")"],"metadata":{"id":"MDe7zIFvNVLj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Fine-tune the model to the custom dataset"],"metadata":{"id":"GxkTTLjuOoNW"}},{"cell_type":"code","source":["trainer.train('/content/drive/MyDrive/NLU Bert Spam Classification/results/checkpoint-400')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":624,"referenced_widgets":["a3935192880e4656a32352a51c2fd58f","f82c231a022e49d89c7d8911f5643f6e","837ad84c194e4e8b867fc878897d25ca","31cb5eb5250942de84c7b2200abf2f9d","3057a124354d45e6ac22913ef9765a7f","68faa7641de1461d94b95a8dc9fec586","f4e799a8c7ab49f3a68bd95e50bcdc6c","1aec608cdf124e64bb38601f875b8b37","6aa89e216f9d4833ab4a621f4320318a","5f94c20d3a1c4004a556bf276f0bea40","796c75d93bd64994b2b6290025df7729"]},"id":"BWkogkcYeecW","outputId":"74f51b8c-6735-4a4c-e14b-a8c69e753ae6","executionInfo":{"status":"ok","timestamp":1682469728391,"user_tz":-60,"elapsed":9178167,"user":{"displayName":"Tony Lay","userId":"12991364819392579390"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Loading model from /content/drive/MyDrive/NLU Bert Spam Classification/results/checkpoint-400.\n","***** Running training *****\n","  Num examples = 3538\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 666\n","  Continuing training from checkpoint, will skip to saved global_step\n","  Continuing training from epoch 1\n","  Continuing training from global step 400\n","  Will skip the first 1 epochs then the first 178 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/178 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3935192880e4656a32352a51c2fd58f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='666' max='666' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [666/666 2:32:00, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.325000</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.299300</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/drive/MyDrive/NLU Bert Spam Classification/results/checkpoint-500\n","Configuration saved in /content/drive/MyDrive/NLU Bert Spam Classification/results/checkpoint-500/config.json\n","Model weights saved in /content/drive/MyDrive/NLU Bert Spam Classification/results/checkpoint-500/pytorch_model.bin\n","Saving model checkpoint to /content/drive/MyDrive/NLU Bert Spam Classification/results/checkpoint-600\n","Configuration saved in /content/drive/MyDrive/NLU Bert Spam Classification/results/checkpoint-600/config.json\n","Model weights saved in /content/drive/MyDrive/NLU Bert Spam Classification/results/checkpoint-600/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=666, training_loss=0.12345781698599234, metrics={'train_runtime': 9169.686, 'train_samples_per_second': 1.158, 'train_steps_per_second': 0.073, 'total_flos': 1406034043508736.0, 'train_loss': 0.12345781698599234, 'epoch': 3.0})"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["# Save the model"],"metadata":{"id":"uSdxkKHGOy-I"}},{"cell_type":"code","source":["trainer.save_model('/content/drive/MyDrive/NLU Bert Spam Classification/results/')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Aq9nWmjzDr-b","executionInfo":{"status":"ok","timestamp":1682470554318,"user_tz":-60,"elapsed":1580,"user":{"displayName":"Tony Lay","userId":"12991364819392579390"}},"outputId":"85c5ad8e-4afb-4433-a233-03bd3f487616"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/drive/MyDrive/NLU Bert Spam Classification/results/\n","Configuration saved in /content/drive/MyDrive/NLU Bert Spam Classification/results/config.json\n","Model weights saved in /content/drive/MyDrive/NLU Bert Spam Classification/results/pytorch_model.bin\n"]}]},{"cell_type":"markdown","source":["# Evaluate the model on the validation set"],"metadata":{"id":"Mvx_3pIrO4DS"}},{"cell_type":"code","source":["# Load saved model and training arguments\n","saved_model = DistilBertForSequenceClassification.from_pretrained(\"/content/drive/MyDrive/NLU Bert Spam Classification/results/model_0\")\n","training_args = torch.load(\"/content/drive/MyDrive/NLU Bert Spam Classification/results/model_0/training_args.bin\")\n","\n","# Define evaluation metric\n","metric = load_metric(\"f1\")\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","    return metric.compute(predictions=predictions, references=labels, average=None)\n","\n","# Retrieve validation set\n","eval_data, eval_labels = create_dataset(DATA_PATH,'combined-test-labelled')\n","eval_encodings = tokenize_data(eval_data)\n","eval_dataset = CustomDataset(eval_encodings, eval_labels)\n","\n","# Evaluate model on the validation set\n","trainer = Trainer(\n","    model=saved_model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset,\n","    compute_metrics=compute_metrics,\n",")\n","\n","trainer.evaluate()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":960},"id":"hc4CB7glL7LG","executionInfo":{"status":"ok","timestamp":1682527107152,"user_tz":-60,"elapsed":588511,"user":{"displayName":"Tony Lay","userId":"12991364819392579390"}},"outputId":"363d5d65-5529-40d3-ff22-a6d3bdad9c80"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file /content/drive/MyDrive/NLU Bert Spam Classification/results/model_0/config.json\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-cased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForSequenceClassification\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"problem_type\": \"single_label_classification\",\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.23.1\",\n","  \"vocab_size\": 28999\n","}\n","\n","loading weights file /content/drive/MyDrive/NLU Bert Spam Classification/results/model_0/pytorch_model.bin\n","All model checkpoint weights were used when initializing DistilBertForSequenceClassification.\n","\n","All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at /content/drive/MyDrive/NLU Bert Spam Classification/results/model_0.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.\n","***** Running Evaluation *****\n","  Num examples = 449\n","  Batch size = 64\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [8/8 08:05]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Trainer is attempting to log a value of \"[0.44444444 0.50340136 0.89235127]\" of type <class 'numpy.ndarray'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"]},{"output_type":"execute_result","data":{"text/plain":["{'eval_loss': 0.6107633709907532,\n"," 'eval_f1': array([0.44444444, 0.50340136, 0.89235127]),\n"," 'eval_runtime': 581.6001,\n"," 'eval_samples_per_second': 0.772,\n"," 'eval_steps_per_second': 0.014}"]},"metadata":{},"execution_count":21}]}]}