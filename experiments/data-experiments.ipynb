{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Reliability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the Data\n",
    "The two datasets that were relabelled are read in into `ann_1` and `ann_2` respectively. Then the corresponding entries from the real dataset are collected and the dataframes are combined to create two dataframes that hold 200 labels made by annotater 1 and annotator 2 individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_1 = pd.read_csv('../data/reliability/sam-reliability-labelled.csv')\n",
    "ann_2 = pd.read_csv('../data/reliability/tony-reliability-labelled.csv')\n",
    "df_ann_1_2 = pd.read_csv('../data/labelled/sam-dataset-labelled.csv')\n",
    "ann_1_2 = df_ann_1_2[df_ann_1_2['comment_id'].isin(ann_2['comment_id'])]\n",
    "df_ann_2_2 = pd.read_csv('../data/labelled/tony-dataset-labelled.csv')\n",
    "ann_2_2 = df_ann_2_2[df_ann_2_2['comment_id'].isin(ann_1['comment_id'])]\n",
    "\n",
    "ann_1 = pd.concat([ann_1, ann_1_2], axis=0)\n",
    "ann_2 = pd.concat([ann_2, ann_2_2], axis=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the Annotations\n",
    "In order to compare the annotations we used **Cohen's Kappa coefficient**. To do this, first we simply counted the following occurences:\n",
    "- When both annotaters labelled a comment as *spam*\n",
    "- When annotater 1 labelled a comment as *spam* and annotator 2 labelled the comment as *ham*\n",
    "- When annotater 1 labelled a comment as *ham* and annotator 2 labelled the comment as *spam*\n",
    "- When both annotaters labelled a comment as *ham*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = ann_1['comment_id'].tolist()\n",
    "spam_spam = 0\n",
    "spam_ham = 0\n",
    "ham_spam = 0\n",
    "ham_ham = 0\n",
    "total = len(ids)\n",
    "for id in ids:\n",
    "    if ann_2.loc[ann_2['comment_id'] == id].empty:\n",
    "        print(\"Not good\")\n",
    "\n",
    "    class_1 = ann_1.loc[ann_1['comment_id'] == id]['class'].tolist()[0]\n",
    "    class_2 = ann_2.loc[ann_2['comment_id'] == id]['class'].tolist()[0]\n",
    "    \n",
    "    if class_1 == \"spam\" and class_2 == \"spam\":\n",
    "        spam_spam += 1\n",
    "    elif class_1 == \"spam\" and class_2 == \"ham\":\n",
    "        spam_ham += 1\n",
    "    elif class_1 == \"ham\" and class_2 == \"spam\":\n",
    "        ham_spam += 1\n",
    "    elif class_1 == \"ham\" and class_2 == \"ham\":\n",
    "        ham_ham += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calculated totals can be used to build a table to see the values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|----------------------------------------------------|\n",
      "|                         |       Annotater 1        |\n",
      "|                         |--------|-------|---------|\n",
      "|                         |  spam  |  ham  |  total  |\n",
      "|-------------------------|--------|-------|---------|\n",
      "|  Annotater 2  |   spam  |   18   |   1   |   19    |\n",
      "|               |---------|--------|-------|---------|\n",
      "|               |   ham   |   1    |  180  |   181   |\n",
      "|               |---------|--------|-------|---------|\n",
      "|               |  total  |   19   |  181  |   200   |\n",
      "|----------------------------------------------------|\n"
     ]
    }
   ],
   "source": [
    "print(f\"|----------------------------------------------------|\")\n",
    "print(f\"|                         |       Annotater 1        |\")\n",
    "print(f\"|                         |--------|-------|---------|\")\n",
    "print(f\"|                         |  spam  |  ham  |  total  |\")\n",
    "print(f\"|-------------------------|--------|-------|---------|\")\n",
    "print(f\"|  Annotater 2  |   spam  |   {spam_spam}   |   {spam_ham}   |   {spam_spam+spam_ham}    |\")\n",
    "print(f\"|               |---------|--------|-------|---------|\")\n",
    "print(f\"|               |   ham   |   {ham_spam}    |  {ham_ham}  |   {ham_spam+ham_ham}   |\")\n",
    "print(f\"|               |---------|--------|-------|---------|\")\n",
    "print(f\"|               |  total  |   {spam_spam + ham_spam}   |  {spam_ham + ham_ham}  |   {total}   |\")\n",
    "print(f\"|----------------------------------------------------|\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the values that we counted, we can plug them into the function for **Cohen's Kappa** to find the values of our *observed agreement*, *expeted agreement*, and *kappa*.\n",
    "\n",
    "\n",
    "#### Cohen's Kappa Formula\n",
    "<img src=\"images/cohens-kappa.png\" alt=\"Cohen's Kappa Formula\">\n",
    "\n",
    "Two annotaters *A1* and *A2*\n",
    "\n",
    "Observed Agreement:\n",
    "*P(a)* = *P(A1=spam, A2=spam)* + *P(A1=ham, A2=ham)*\n",
    "\n",
    "Expected Agreement:\n",
    "*P(e)* = *P(A1=spam)* * *P(A2=spam)* + *P(A1=ham)* * *P(A2=ham)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_agreement = (spam_spam/total) + (ham_ham/total)\n",
    "expected_agreeement = (((spam_spam + spam_ham)/total) * ((spam_spam + ham_spam)/total)) + (((ham_spam + ham_ham)/total) * ((spam_ham + ham_ham)/total))\n",
    "kappa = (observed_agreement - expected_agreeement) / (1 - expected_agreeement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed Agreement: 0.99\n",
      "Expected Agreement: 0.82805\n",
      "Kappa: 0.9418435591741785\n"
     ]
    }
   ],
   "source": [
    "print(f\"Observed Agreement: {observed_agreement}\")\n",
    "print(f\"Expected Agreement: {expected_agreeement}\")\n",
    "print(f\"Kappa: {kappa}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "**Landis and Koch, 1977** came up with the following interpretation of the coefficient. Their interpretation was:\n",
    "\n",
    "*slight < 0.2 < fair < 0.4 < moderate < 0.6 < substantial < 0.8 < perfect*\n",
    "\n",
    "*Others also state that any value above around 0.8 is a high agreement and shows that the data is reliable.* \n",
    "\n",
    "We obtained a value of **0.94**, showing a high agreement between the annotaters. From this we can safely state that the agreement on what is and what is not spam for our dataset is high and we can safely use this to train a neural network, without introducing contradiction to the network.\n",
    "\n",
    "Landis, J. R., & Koch, G. G. (1977). The Measurement of Observer Agreement for Categorical Data. Biometrics, 33(1), 159â€“174. https://doi.org/10.2307/2529310"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
